{{- if .Values.components.otel.enabled }}
{{- /* Create ConfigMap for OpenShift trusted CA bundle injection */}}
{{- if and $.Values.openshift $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-trusted-cabundle
  namespace: {{ .Release.Namespace }}
  labels:
    # OpenShift injects trusted CA bundle into ConfigMaps with this label
    config.openshift.io/inject-trusted-cabundle: "true"
    {{- include "kagenti.labels" . | nindent 4 }}
data: {}
{{- end }}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.122.1
          command:
          - /otelcol-contrib
          - --config=/etc/otelcol-config/base.yaml
          - --set
          - receivers::otlp::protocols::http::endpoint=0.0.0.0:8335
          {{- if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
          # TODO: OAuth tokens are required because MLflow's mlflow-oidc-auth plugin doesn't support
          # mTLS/mesh authentication. The plugin only validates OIDC tokens, not client certificates.
          # This means the otel-collector can't use Istio Ambient mesh identity to push traces to MLflow.
          # The secret is created by mlflow-oauth-secret-job in the kagenti chart (installed after kagenti-deps).
          # If otel-collector starts before the secret exists, it will start with empty credentials.
          # Restart the otel-collector deployment after kagenti chart installs to pick up the credentials.
          env:
            - name: MLFLOW_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_CLIENT_ID
                  optional: true
            - name: MLFLOW_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_CLIENT_SECRET
                  optional: true
            - name: KEYCLOAK_TOKEN_URL
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_TOKEN_URL
                  optional: true
          {{- end }}
          volumeMounts:
            - name: otel-collector-config
              mountPath: /etc/otelcol-config
            {{- if and $.Values.openshift $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
            - name: trusted-ca
              mountPath: /etc/pki/ca-trust/extracted/pem
              readOnly: true
            - name: ingress-ca
              mountPath: /etc/pki/ingress-ca
              readOnly: true
            {{- end }}
          ports:
            - containerPort: 4317
              name: otlp-grpc
              protocol: TCP
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 8335
              name: otlp-receiver
              protocol: TCP
      volumes:
        - name: otel-collector-config
          configMap:
            name: otel-collector-config
        {{- if and $.Values.openshift $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
        # Use OpenShift's pre-existing trusted CA bundle ConfigMap
        - name: trusted-ca
          configMap:
            name: config-trusted-cabundle
            items:
              - key: ca-bundle.crt
                path: tls-ca-bundle.pem
        # OpenShift ingress CA for external Keycloak HTTPS
        # Copied from openshift-config-managed/default-ingress-cert by pre-install Job
        - name: ingress-ca
          configMap:
            name: otel-ingress-ca
            items:
              - key: ca-bundle.crt
                path: ingress-ca.pem
        {{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      protocol: TCP
      port: 4317
      targetPort: 4317
    - name: otlp-http
      protocol: TCP
      port: 4318
      targetPort: 4318
    - name: otlp-receiver
      protocol: TCP
      port: 8335
      targetPort: 8335
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kagenti.labels" . | nindent 4 }}
data:
  base.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318 # overriden by the server

    exporters:
      debug:
        verbosity: detailed
      otlp/phoenix:
        endpoint: phoenix:4317
        tls:
          insecure: true
      {{- if $.Values.components.mlflow.enabled }}
      otlphttp/mlflow:
        # Use traces_endpoint to specify exact URL (endpoint would append /v1/traces)
        traces_endpoint: http://mlflow:5000/v1/traces
        tls:
          insecure: true
        headers:
          # Default experiment ID (0 = Default experiment)
          # Traces can override via x-mlflow-experiment-id attribute
          x-mlflow-experiment-id: "0"
        # Retry on failure - MLflow may return 503 during startup
        # while database migrations and init are running (~100s)
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
        sending_queue:
          enabled: true
          num_consumers: 2
          queue_size: 1000
        {{- if $.Values.mlflow.auth.enabled }}
        # Authenticate to MLflow using OAuth2 client credentials via Keycloak
        auth:
          authenticator: oauth2client/mlflow
        {{- end }}
      {{- end }}

    processors:
      memory_limiter:
        check_interval: 1s
        limit_mib: 1000
      batch:
      filter/phoenix:
        traces:
          span:
            ## Filter OUT A2A framework spans (span names starting with "a2a.")
            ## Keep: OpenInference, LangChain, CrewAI, and GenAI agent spans
            - IsMatch(name, "^a2a\\..*")
      # Transform GenAI semantic conventions to OpenInference format for Phoenix
      # See: https://opentelemetry.io/docs/specs/semconv/gen-ai/
      # See: https://github.com/Arize-ai/openinference
      transform/genai_to_openinference:
        trace_statements:
          - context: span
            statements:
              # Convert GenAI model name to OpenInference format
              - set(attributes["llm.model_name"], attributes["gen_ai.request.model"]) where attributes["gen_ai.request.model"] != nil
              - set(attributes["llm.model_name"], attributes["gen_ai.response.model"]) where attributes["gen_ai.response.model"] != nil and attributes["llm.model_name"] == nil
              # Convert GenAI token counts to OpenInference format
              - set(attributes["llm.token_count.prompt"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["llm.token_count.completion"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
              - set(attributes["llm.token_count.total"], attributes["gen_ai.usage.input_tokens"] + attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil and attributes["gen_ai.usage.output_tokens"] != nil
              # Convert GenAI system to OpenInference provider
              - set(attributes["llm.provider"], attributes["gen_ai.system"]) where attributes["gen_ai.system"] != nil
              - set(attributes["llm.system"], attributes["gen_ai.system"]) where attributes["gen_ai.system"] != nil
              # Convert GenAI invocation parameters (temperature)
              - set(attributes["llm.invocation_parameters"], Concat(["{\"temperature\":", Concat([attributes["gen_ai.request.temperature"], "}"], "")], "")) where attributes["gen_ai.request.temperature"] != nil
      # Enrich agent spans with MLflow, OpenInference, and GenAI attributes.
      # Agents only set input.value, output.value, gen_ai.agent.name,
      # gen_ai.operation.name, and gen_ai.conversation.id.
      # This processor derives all other observability attributes from those.
      # Root span detection: IsMatch(name, "^invoke_agent.*")
      transform/agent_enrichment:
        trace_statements:
          - context: span
            statements:
              # === MLflow attributes (from existing span + resource attrs) ===
              # Copy input.value to mlflow.spanInputs for MLflow display
              - set(attributes["mlflow.spanInputs"], attributes["input.value"]) where attributes["input.value"] != nil
              # Copy output.value to mlflow.spanOutputs
              - set(attributes["mlflow.spanOutputs"], attributes["output.value"]) where attributes["output.value"] != nil
              # Set trace name from agent name
              - set(attributes["mlflow.traceName"], attributes["gen_ai.agent.name"]) where attributes["gen_ai.agent.name"] != nil
              # Set source from resource service.name
              - set(attributes["mlflow.source"], resource.attributes["service.name"]) where resource.attributes["service.name"] != nil
              # Set version from resource service.version
              - set(attributes["mlflow.version"], resource.attributes["service.version"]) where resource.attributes["service.version"] != nil
              # Mark agent spans as AGENT type for MLflow
              - set(attributes["mlflow.spanType"], "AGENT") where IsMatch(name, "^invoke_agent.*")
              # Set run name
              - set(attributes["mlflow.runName"], Concat([attributes["gen_ai.agent.name"], "-invoke"], "")) where attributes["gen_ai.agent.name"] != nil
              # Default user
              - set(attributes["mlflow.user"], "kagenti") where attributes["mlflow.user"] == nil and IsMatch(name, "^invoke_agent.*")
              # Session tracking
              - set(attributes["mlflow.trace.session"], attributes["gen_ai.conversation.id"]) where attributes["gen_ai.conversation.id"] != nil
              # === OpenInference attributes ===
              # Set span kind to AGENT for root spans
              - set(attributes["openinference.span.kind"], "AGENT") where IsMatch(name, "^invoke_agent.*")
              # === GenAI to OpenInference conversion (for Phoenix) ===
              - set(attributes["llm.model_name"], attributes["gen_ai.request.model"]) where attributes["gen_ai.request.model"] != nil
              - set(attributes["llm.token_count.prompt"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["llm.token_count.completion"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
      {{- if $.Values.components.mlflow.enabled }}
      # Filter for MLflow: only keep agent spans, filter out A2A framework spans
      # A2A framework spans have names starting with "a2a."
      filter/mlflow:
        traces:
          span:
            # Filter OUT spans that match these conditions (A2A framework spans):
            # - Span names starting with "a2a." (A2A server framework)
            # Keep: spans with agent attributes, LangChain, OpenInference
            - IsMatch(name, "^a2a\\..*")
      {{- end }}

    extensions:
      health_check:
      {{- if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
      oauth2client/mlflow:
        # OAuth2 client credentials for authenticating to MLflow
        client_id: ${env:MLFLOW_CLIENT_ID}
        client_secret: ${env:MLFLOW_CLIENT_SECRET}
        token_url: ${env:KEYCLOAK_TOKEN_URL}
        scopes: ["openid"]
        # Timeout for token requests
        timeout: 10s
        {{- if $.Values.openshift }}
        # Use OpenShift ingress CA for external Keycloak HTTPS endpoint
        # The ingress CA is copied from openshift-config-managed/default-ingress-cert
        tls:
          ca_file: /etc/pki/ingress-ca/ingress-ca.pem
        {{- end }}
      {{- end }}

    service:
      extensions: [ health_check{{ if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}, oauth2client/mlflow{{ end }} ]
      pipelines:
        traces/phoenix:
          receivers: [ otlp ]
          processors: [ memory_limiter, filter/phoenix, transform/agent_enrichment, transform/genai_to_openinference, batch ]
          exporters: [ otlp/phoenix ]
        {{- if $.Values.components.mlflow.enabled }}
        traces/mlflow:
          receivers: [ otlp ]
          processors: [ memory_limiter, filter/mlflow, transform/agent_enrichment, batch ]
          exporters: [ debug, otlphttp/mlflow ]
        {{- end }}
{{- end }}
